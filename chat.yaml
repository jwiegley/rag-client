# RAG Configuration
query:
  # Primary LLM for querying. Choose one of the types below:
  llm: &llm
    type: OpenAILikeConfig
    model: "hera/Qwen3-235B-A22B-Instruct-2507"       # Model name to use
    api_base: "http://vulcan/litellm/v1" # API endpoint
    api_key_command: "pass vulcan | head -1"
    context_window: 131072               # Maximum context window size
    temperature: 0.6                     # Sampling temperature for generation
    max_tokens: 8192                     # Max output tokens
    timeout: 7200                        # API timeout in seconds
    add_litellm_session_id: true
    default_headers:
      x-litellm-tags: rag-client

    # type: OllamaConfig
    # model: "llama3"
    # base_url: "http://localhost:11434"
    # keep_alive: "5m"            # How long to keep model loaded (Ollama specific)

    # type: LlamaCPPConfig
    # model_url: "https://example.com/model.gguf"  # Model download URL
    # model_path: "models/llama3.gguf"             # Local model path
    # verbose: true                                # Verbose output for debugging

    # type: PerplexityConfig
    # model: "pplx-xxl"
    # api_base: "https://api.perplexity.ai"

    # type: OpenRouterConfig
    # model: "meta-llama/llama-3-70b"

    # type: LMStudioConfig
    # model_name: "lmstudio-ai/Qwen3-235B-A22B"
    # base_url: "http://localhost:1234/v1"

  # Query engine configuration. Choose one engine type:
  engine:
    # type: SimpleQueryEngineConfig       # Stateless Q&A engine

    type: RetrieverQueryEngineConfig
    response_mode: compact_accumulate     # Options: 'default', 'compact',
                                          # 'compact_accumulate', 'refine'

    # type: CitationQueryEngineConfig
    # chunk_size: 512                     # Content chunk size for citation
    # chunk_overlap: 20                   # Overlap between chunks

    # type: RetryQueryEngineConfig
    # evaluator:
    #   type: RelevancyEvaluatorConfig
    #   llm: *llm                         # LLM for evaluating responses
    # engine:
    #   type: SimpleQueryEngineConfig

    # type: RetrySourceQueryEngineConfig
    # evaluator:
    #   type: GuidelineConfig
    #   llm: *llm
    #   guidelines: "Ensure response contains proper citations"
    # engine:
    #   type: SimpleQueryEngineConfig

chat:
  # Primary LLM for chat. Can match query.llm or be different.
  llm: *llm

  default_user: "user"                    # Default user identifier
  summarize: false                        # Summarize long chat histories
  keep_history: true                      # Persist chat history between sessions

  engine:
    type: SimpleContextChatEngineConfig
    context_window: 32768                 # Max tokens for context window

    # type: CondensePlusContextChatEngineConfig
    # skip_condense: false                # Whether to skip condensing user message

    # type: ContextChatEngineConfig

retrieval:
  # Document retrieval configuration
  embed_individually: false

  # Fusion retriever for multi-strategy search
  # fusion:
  #   type: FusionRetrieverConfig
  #   num_queries: 3                      # Number of query transformations
  #   mode: "relative_score"              # Fusion mode: 'relative_score' or 'reciprocal_rank'
  #   llm: *llm

  # Vector store configuration (Postgres or local)
  # vector_store:
  #   type: PostgresVectorStoreConfig
  #   connection: "postgresql://user:pass@localhost:5432/db"
  #   hybrid_search: true                 # Enable hybrid search
  #   dimensions: 4096                    # Embedding vector dimensions
  #   hnsw_m: 16                          # HNSW graph connections
  #   hnsw_ef_construction: 64            # HNSW index quality
  #   hnsw_ef_search: 40                  # Search quality parameter
  #   hnsw_dist_method: "vector_l2_ops"   # Distance calculation method

  # Embedding model configuration. Choose one:
  embedding:
    type: OpenAILikeEmbeddingConfig
    model_name: "hera/bge-m3"           # Model identifier
    # dimensions: 1024                    # Embedding dimensions
    # model_name: "Qwen3-Embedding-8B"
    # dimensions: 4096
    # model_name: "hera/nomic-embed-text-v2-moe"
    # dimensions: 768
    # model_name: "sentence-transformers/all-MiniLM-L6-v2"
    # dimensions: 384
    api_base: "http://vulcan/litellm/v1"  # API endpoint
    api_key_command: "pass vulcan | head -1"
    # api_base: "http://localhost:8080/v1"  # API endpoint
    # api_key: "fake"
    no_litellm_logging: true
    add_litellm_session_id: true
    default_headers:
      x-litellm-tags: rag-client

    # type: HuggingFaceEmbeddingConfig
    # model_name: "BAAI/bge-base-en-v1.5" # Huggingface model name
    # max_length: 512                     # Max tokens per batch
    # normalize: true                     # Normalize vectors

    # type: OllamaEmbeddingConfig
    # model_name: "nomic-embed-text"      # Ollama model name

    # type: LlamaCPPEmbeddingConfig
    # model_path: "models/bge.gguf"       # Local model path
    # n_gpu_layers: 99                    # Layers to offload to GPU
    # verbose: true                       # Verbose output for debugging

    # type: OpenAIEmbeddingConfig
    # mode: "text_search"                 # Embedding mode: 'text_search' or 'text_embedding'
    # model: "text-embedding-ada-002"     # Model type

  # Text splitting strategy. Choose one:
  splitter:
    type: SentenceSplitterConfig
    chunk_size: 512                       # Characters per chunk
    chunk_overlap: 20                     # Overlap between chunks
    include_metadata: true                # Preserve metadata in chunks

    # type: SentenceWindowSplitterConfig
    # window_size: 5                      # Number of sentences to include
    # window_metadata_key: "window"       # Where to store window content
    # original_text_metadata_key: "original" # Where to store original text

    # type: SemanticSplitterConfig
    # embedding: *embedding               # Reference to embedding model
    # buffer_size: 200                    # Tokens to consider for splitting
    # breakpoint_percentile_threshold: 95 # Semantic breakpoint threshold

    # type: JSONNodeParserConfig
    # include_metadata: true
    # include_prev_next_rel: true

    # type: CodeSplitterConfig
    # language: "python"                  # Programming language
    # chunk_lines: 50                     # Lines per chunk
    # chunk_lines_overlap: 15             # Overlapping lines

  # Node metadata extractors. Multiple can be used:
  extractors: []
    # - type: KeywordExtractorConfig
    #   keywords: 5                       # Number of keywords to extract
    #   llm: *llm
    #   # extractor_name: "keywords"      # Custom name for the extractor

    # - type: SummaryExtractorConfig
    #   summaries: ["self", "query"]      # Types of summaries to extract
    #   llm: *llm

    # - type: TitleExtractorConfig
    #   nodes: 5                          # Number of title candidates
    #   llm: *llm

    # - type: QuestionsAnsweredExtractorConfig
    #   questions: 3                      # Number of questions to extract
    #   llm: *llm

  # Keyword index configuration (for keyword-based search)
  # keywords:
  #   collect: true                       # Whether to build keyword index
  #   llm: *llm                           # LLM for keyword extraction
