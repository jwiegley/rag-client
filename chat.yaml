query:
  llm: &llm
    type: OpenAILikeConfig
    model: "Llama-4-Maverick-17B-128E-Instruct"
    api_base: "http://localhost:8080/v1"
    # api_base: "http://192.168.50.5:8080/v1"
    temperature: 0.7
    max_tokens: 1500
    timeout: 7200
  source_retries: false

chat:
  llm: *llm
  # default_user: "user1"
  # summarize: False
  engine:
    type: CondensePlusContextChatEngineConfig
    skip_condense: false

retrieval:
  top_k: 40
  sparse_top_k: 5

  fusion:
    llm: *llm

  # vector_store:
  #   connection: "postgresql+psycopg2://postgres@localhost:5432/vector_db"
  #   hybrid_search: true

  # embedding:
  #   provider: "BGEM3"
  #   model: "BAAI/bge-m3"
  #   dimensions: 1024
  embedding:
    type: HuggingFaceEmbeddingConfig
    model_name: "BAAI/bge-large-en-v1.5"
    # model_name: "BAAI/bge-base-en-v1.5"

  splitter:
    type: SentenceSplitterConfig
    chunk_size: 512
    chunk_overlap: 20

  extractors: []
    # - type: KeywordExtractorConfig
    #   keywords: 5
    #   llm:
    #     <<: *llm
    #     max_tokens: 100
    # - type: SummaryExtractorConfig
    #   summaries:
    #     - "self"
    #   llm: *llm
    # - type: TitleExtractorConfig
    #   nodes: 5
    #   llm: *llm
    # - type: QuestionsAnsweredExtractorConfig
    #   questions: 3
    #   llm: *llm

  # keywords:
  #   collect: true
  #   llm: *llm
